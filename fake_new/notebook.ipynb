{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Analysis of fakenews project - CS 371\n",
    "Author: Alex Nguyen and Hao Lin | Gettysburg College\n",
    "\n",
    "## Structure of the repository\n",
    "\n",
    "* The [`README.md`](./README.md) is the main written answer file that reader should follow in addition to the code written in [`notebook.py`](./notebook.py) and [`notebook.ipynb`](./notebook.ipynb).\n",
    "\n",
    "* The [analysis folder](./analysis) contains csv files that analyze the nature of the data.\n",
    "\n",
    "* The [data folder](./data) contains the data files.\n",
    "\n",
    "* The file [`notebook.py`](./notebook.py) is the main file that contains the main code analysis\n",
    "\n",
    "* The file [`notebook.ipynb`](./notebook.ipynb) is the main notebook that contains the main code analysis\n",
    "\n",
    "* [Here](https://colab.research.google.com/drive/1CniVqlrgH_wxul13CTXURzmqkxM3zVH8?usp=sharing) is the editable link to the google colab jupyter notebook. <b>Note:</b> Please upload the required folders and data files in order for the notebook to work."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# CONFIGURATIONs\n",
    "\n",
    "DATA_FOLDER = Path(\"./data\")\n",
    "FAKE_DATA_NAME = \"clean_fake.txt\"\n",
    "FAKE_DATA_PATH = DATA_FOLDER / FAKE_DATA_NAME\n",
    "REAL_DATA_NAME = \"clean_real.txt\"\n",
    "REAL_DATA_PATH = DATA_FOLDER / REAL_DATA_NAME"
   ]
  },
  {
   "source": [
    "First we define our methods:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(real_path, fake_path, stop_words : List[str]=[]):\n",
    "    \"\"\"\n",
    "        Given a path to real data and fake data, return 2 2d-array of word \n",
    "        with shape(n_sentences, n_words_in_sentence)\n",
    "    \"\"\"\n",
    "    fake: List[str] = []\n",
    "    with open (fake_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            tmp = []\n",
    "            for w in line.strip().split():\n",
    "                if w not in stop_words:\n",
    "                    tmp.append(w)\n",
    "            fake.append(tmp)\n",
    "    real: List[str] = []\n",
    "    with open (real_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            tmp = []\n",
    "            for w in line.strip().split():\n",
    "                if w not in stop_words:\n",
    "                    tmp.append(w)\n",
    "            real.append(tmp)\n",
    "    return np.asarray(real), np.asarray(fake)\n",
    "\n",
    "# Get the most presence word\n",
    "def export_most_presence_words(real_x: List[str], fake_x: List[str]):\n",
    "    total_real = 0\n",
    "    cnt_real = collections.Counter()\n",
    "    for line in real_x:\n",
    "        words = line.split()\n",
    "        for w in words:\n",
    "            cnt_real[w] += 1\n",
    "            total_real += 1\n",
    "\n",
    "    cnt_fake = collections.Counter()\n",
    "    for line in fake_x:\n",
    "        words = line.split()\n",
    "        for w in words:\n",
    "            cnt_fake[w] += 1\n",
    "\n",
    "    most_presence = np.asarray(cnt_real.most_common(10))\n",
    "    # print(most_presence.shape)\n",
    "    most_presence_probs = [int(data) / total_real for data in most_presence[:,1]]\n",
    "    print(\"Most presence words probability:\\n\" + str(most_presence_probs))\n",
    "    least_presence = np.asarray(cnt_real.most_common()[:-10-1:-1])\n",
    "    least_presence_probs = [int(data) / total_real for data in least_presence[:,1]]\n",
    "    print(\"Least presence words probability:\\n\" + str(least_presence_probs))\n",
    "\n",
    "    # Write csv\n",
    "    df = pd.DataFrame(cnt_fake.items())\n",
    "    df.to_csv(\"./analysis/fake_words.csv\")\n",
    "\n",
    "    df = pd.DataFrame(cnt_real.items())\n",
    "    df.to_csv(\"./analysis/real_words.csv\")\n",
    "\n",
    "# Get the most presence word\n",
    "def export_most_presence_words_with_stop_words(real_x: List[str], fake_x: List[str]):\n",
    "    cnt_real_non_stop = collections.Counter()\n",
    "    for line in real_x:\n",
    "        words = line.split()\n",
    "        for w in words:\n",
    "            if w not in ENGLISH_STOP_WORDS:\n",
    "                cnt_real_non_stop[w] += 1\n",
    "\n",
    "    cnt_fake_non_stop = collections.Counter()\n",
    "    for line in fake_x:\n",
    "        words = line.split()\n",
    "        for w in words:\n",
    "            if w not in ENGLISH_STOP_WORDS:\n",
    "                cnt_fake_non_stop[w] += 1\n",
    "\n",
    "    # Write csv\n",
    "    df = pd.DataFrame(cnt_fake_non_stop.items())\n",
    "    df.to_csv(\"./analysis/fake_words_non_stop.csv\")\n",
    "\n",
    "    df = pd.DataFrame(cnt_real_non_stop.items())\n",
    "    df.to_csv(\"./analysis/real_words_non_stop.csv\")"
   ]
  },
  {
   "source": [
    "## Part 1: \n",
    "- Describe the datasets. You will be predicting whether a headline is real or fake news from words that appear in the headline. Is that feasible? Give 3 examples of specific keywords that may be useful, together with statistics on how often they appear in real and fake headlines.\n",
    "- For the rest of the project, you should split your dataset into ~70% training, ~15% validation, and ~15% test.\n",
    "\n",
    "<b>Answer:</b>\n",
    "* According to [`fake_word_non_stop.csv`](./analysis/fake_word_non_stop.csv) (all distinct words in the fake new and its occurrence that does not include the stop words), we can see that the most frequent keys is \"Trump\", \"Donald\", and \"Hilary\".\n",
    "* The data was splited in the train, test, and validationin set in [`notebook.py`](./notebook.py)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Part 2: Create Naive Bayes classifier\n",
    "## Part 5 and 6: Create and analyze Logistic Regression.\n",
    "## Part 7: Create and analyze Decision Tree\n",
    "\n",
    "### Note: All this models are trained on data CONTAINING stop-words."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing accuracy for Naive Bayes = 0.8591836734693877\nTesting accuracy Logistic Regression = 0.8673469387755102\n\nLargest 10 coefficient for logistic regression: [1.6945726991574033, 1.5952871127753492, 1.5491104095088786, 1.37658765749825, 1.3200212833377263, 1.2346420108946954, 1.1783898230422958, 1.1772149318892855, 1.1692347159558547, 1.1361404508257504]\n\nSmallest 10 coefficient for logistic regression: [-1.9089776914282195, -1.755714622009212, -1.502867042717414, -1.4997133850831266, -1.3971909247765497, -1.3886643883888463, -1.326854464596621, -1.3225371306974745, -1.3197105974382202, -1.263507260612528]\n\nTesting accuracy Decision Tree classifier = 0.7795918367346939\n"
     ]
    }
   ],
   "source": [
    "real_x, fake_x = read_data(REAL_DATA_PATH, FAKE_DATA_PATH)\n",
    "\n",
    "real_lines = np.asarray([\" \".join(row) for row in real_x])\n",
    "fake_lines = np.asarray([\" \".join(row) for row in fake_x])\n",
    "real_y = np.asarray(len(real_lines) * [1])\n",
    "fake_y = np.asarray(len(fake_lines) * [0])\n",
    "\n",
    "data_lines = np.append(real_lines, fake_lines, axis=0)\n",
    "data_label = np.append(real_y, fake_y, axis=0)\n",
    "\n",
    "random.seed(0)\n",
    "random.shuffle(data_lines)\n",
    "random.seed(0)\n",
    "random.shuffle(data_label)\n",
    "\n",
    "# # SPlit train and other parts!\n",
    "x_train, x_, y_train, y_ = train_test_split(data_lines, data_label, test_size=0.3)\n",
    "# # Split test and validation for those parts!\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_, y_, test_size=0.5)\n",
    "\n",
    "# Vectorize train test\n",
    "vectorizer = CountVectorizer()\n",
    "counts_train = vectorizer.fit_transform(x_train)\n",
    "counts_test = vectorizer.transform(x_test)\n",
    "\n",
    "# Naive Bayes\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(counts_train, y_train)\n",
    "predictions = classifier.predict(counts_test)\n",
    "print('Testing accuracy for Naive Bayes =', sum(predictions == y_test) / len(y_test))\n",
    "\n",
    "# Logistic Regression\n",
    "regression = LogisticRegression()\n",
    "regression.fit(counts_train, y_train)\n",
    "predictions = regression.predict(counts_test)\n",
    "print('Testing accuracy Logistic Regression =', sum(predictions == y_test) / len(y_test))\n",
    "\n",
    "coef = np.asarray(regression.coef_)\n",
    "coef = coef.flatten()\n",
    "# Getting the n (10) largest coefficients of the logistic regression.\n",
    "max_args = (-coef).argsort()[:10]\n",
    "max_probs = [coef[arg] for arg in max_args]\n",
    "# Getting the n (10) smallest coefficients of the logistic regression.\n",
    "min_args = (coef).argsort()[:10]\n",
    "min_probs = [coef[arg] for arg in min_args]\n",
    "\n",
    "print()\n",
    "print(\"Largest 10 coefficient for logistic regression: \" + str(max_probs) + \"\\n\")\n",
    "print(\"Smallest 10 coefficient for logistic regression: \" + str(min_probs))\n",
    "print()\n",
    "\n",
    "# Decision tree classifier with normal data\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(counts_train, y_train)\n",
    "predictions = dtree.predict(counts_test)\n",
    "print('Testing accuracy Decision Tree classifier =', sum(predictions == y_test) / len(y_test))"
   ]
  },
  {
   "source": [
    "* Now try classify the data with non stop words:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing accuracy for Naive Bayes = 0.8448979591836735\n",
      "Testing accuracy Logistic Regression = 0.8428571428571429\n",
      "\n",
      "Largest 10 coefficient for logistic regression: [1.5154451202658934, 1.5133845233359218, 1.4489570956531233, 1.3610718727678532, 1.336908467209301, 1.3188928359405674, 1.282321519151832, 1.2663677690919637, 1.2576864286655547, 1.1212793508954753]\n",
      "\n",
      "Smallest 10 coefficient for logistic regression: [-1.9387706777634726, -1.6237100196063001, -1.610085723416201, -1.55157132862154, -1.5461239219035725, -1.5319972558210335, -1.3282052941479423, -1.279801907856682, -1.2629452649977808, -1.261497226060493]\n",
      "\n",
      "Testing accuracy Decision Tree classifier = 0.789795918367347 \n",
      "\n",
      "Trying different depth from 1 to 100...\n",
      "Testing accuracy Decision Tree classifier for depth 48 = 0.7714285714285715\n",
      "Testing accuracy Decision Tree classifier for depth 99 = 0.7959183673469388\n",
      "Testing accuracy Decision Tree classifier for depth 12 = 0.6755102040816326\n",
      "Testing accuracy Decision Tree classifier for depth 50 = 0.7795918367346939\n",
      "Testing accuracy Decision Tree classifier for depth 3 = 0.636734693877551\n",
      "Testing accuracy Decision Tree classifier for depth 32 = 0.7510204081632653\n",
      "Testing accuracy Decision Tree classifier for depth 95 = 0.7857142857142857\n",
      "Testing accuracy Decision Tree classifier for depth 38 = 0.7510204081632653\n",
      "Testing accuracy Decision Tree classifier for depth 39 = 0.7612244897959184\n",
      "Testing accuracy Decision Tree classifier for depth 32 = 0.7510204081632653\n"
     ]
    }
   ],
   "source": [
    "# Redo the whole process with non-stop-word\n",
    "\n",
    "real_x, fake_x = read_data(REAL_DATA_PATH, FAKE_DATA_PATH, ENGLISH_STOP_WORDS)\n",
    "\n",
    "real_lines = np.asarray([\" \".join(row) for row in real_x])\n",
    "fake_lines = np.asarray([\" \".join(row) for row in fake_x])\n",
    "real_y = np.asarray(len(real_lines) * [1])\n",
    "fake_y = np.asarray(len(fake_lines) * [0])\n",
    "\n",
    "data_lines = np.append(real_lines, fake_lines, axis=0)\n",
    "data_label = np.append(real_y, fake_y, axis=0)\n",
    "\n",
    "random.seed(0)\n",
    "random.shuffle(data_lines)\n",
    "random.seed(0)\n",
    "random.shuffle(data_label)\n",
    "\n",
    "# # SPlit train and other parts!\n",
    "x_train, x_, y_train, y_ = train_test_split(data_lines, data_label, test_size=0.3)\n",
    "# # Split test and validation for those parts!\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_, y_, test_size=0.5)\n",
    "\n",
    "# Vectorize train test\n",
    "vectorizer = CountVectorizer()\n",
    "counts_train = vectorizer.fit_transform(x_train)\n",
    "counts_test = vectorizer.transform(x_test)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(counts_train, y_train)\n",
    "predictions = classifier.predict(counts_test)\n",
    "print('Testing accuracy for Naive Bayes =', sum(predictions == y_test) / len(y_test))\n",
    "\n",
    "regression = LogisticRegression()\n",
    "regression.fit(counts_train, y_train)\n",
    "predictions = regression.predict(counts_test)\n",
    "print('Testing accuracy Logistic Regression =', sum(predictions == y_test) / len(y_test))\n",
    "\n",
    "coef = np.asarray(regression.coef_)\n",
    "coef = coef.flatten()\n",
    "max_args = (-coef).argsort()[:10]\n",
    "max_probs = [coef[arg] for arg in max_args]\n",
    "min_args = (coef).argsort()[:10]\n",
    "min_probs = [coef[arg] for arg in min_args]\n",
    "\n",
    "print()\n",
    "print(\"Largest 10 coefficient for logistic regression: \" + str(max_probs) + \"\\n\")\n",
    "print(\"Smallest 10 coefficient for logistic regression: \" + str(min_probs))\n",
    "print()\n",
    "\n",
    "# Decision tree classifier for nonstop word\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(counts_train, y_train)\n",
    "predictions = dtree.predict(counts_test)\n",
    "print('Testing accuracy Decision Tree classifier =', sum(predictions == y_test) / len(y_test), \"\\n\")\n",
    "\n",
    "print(\"Trying different depth from 1 to 100...\")\n",
    "for i in range(10):\n",
    "    max_depth = np.random.randint(1,100)\n",
    "    dtree = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    dtree.fit(counts_train, y_train)\n",
    "    predictions = dtree.predict(counts_test)\n",
    "    t = sum(predictions == y_test) / len(y_test)\n",
    "    print(f'Testing accuracy Decision Tree classifier for depth {max_depth} = {t}')\n"
   ]
  },
  {
   "source": [
    "## Part 3:\n",
    "### Part 3a:\n",
    "* According to the [`real_words.csv`](./analysis/real_words.csv), sorting the csv file in ascending and descending order in the number of presence gives us the least and most frequent keywords, respectively:\n",
    "  * The most frequent keywords in real news: 'donald', 'to', 'us', 'trumps', 'in', 'on', 'of', 'for', 'the'.\n",
    "  * The least frequent keywords in real news: 'ba', 'how', 'climate', 'obama', 'house', 'has', 'first', 'he', 'not', 'what'.\n",
    "\n",
    "* According to the [`fake_words.csv`](./analysis/fake_words.csv), sorting the csv file in ascending and descending order in the number of presence gives us the least and most frequent keywords, respectively:\n",
    "  * The most frequent keywords in fake news: 'trump', 'the', 'to', 'in', 'donald', 'of', 'for', 'a', 'and', 'on'.\n",
    "\n",
    "  * The least frequent keywords in fake news: 'why', 'after', 'campaign', 'america', 'voter', 'vote', 'not', 'supporter', 'about', 'says'.\n",
    "\n",
    "### Part 3b:\n",
    "* According to the [`real_words_non_stop.csv`](./analysis/real_words_non_stop.csv), sorting the csv file in descending order in the number of presence gives us the most frequent keywords:\n",
    "  * The most frequent keywords in real news: 'donald', 'trumps', 'says', 'trum', 'north', 'election', 'clinton', 'president', 'russia', 'korea'.\n",
    "\n",
    "* According to the [`fake_words_non_stop.csv`](./analysis/fake_words_non_stop.csv), sorting the csv file in descending order in the number of presence gives us the most frequent keywords:\n",
    "  * The most frequent keywords in fake news: 'trump', 'donald', 'hillary', 'clinton', 'trum', 'new', 'just', 'election', 'obama', 'president'\n",
    "\n",
    "\n",
    "### Part 3c:\n",
    "* It is important to remove stop words from the model because stop words are not relevant to the main content or are not the strong inference of whether to classify a headline as real or fake. Therefore, it should be removed from the dataset.\n",
    "* It is important to keep stop words from the model because some particular stop words appears to make a headline looks more professional, defining the credibility of the headline of the article.\n",
    "\n",
    "The code to achieve these results is below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_most_presence_words(real_lines, fake_lines)\n",
    "export_most_presence_words_with_stop_words(real_lines, fake_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}