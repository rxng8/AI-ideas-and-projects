{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmVzsRlDc7jj"
      },
      "source": [
        "!wget http://vision.ucsd.edu/datasets/yale_face_dataset_original/yalefaces.zip\r\n",
        "!unzip yalefaces.zip\r\n",
        "!mkdir dataset\r\n",
        "!mv yalefaces/ ./dataset\r\n",
        "!rm -rf _MACOSX/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEf8rT2pa62G"
      },
      "source": [
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "import numpy as np\n",
        "import pywt\n",
        "import pywt.data\n",
        "from PIL import Image\n",
        "import math\n",
        "\n",
        "# import matplotlib.pylab as plt\n",
        "from skimage.io import imread\n",
        "from skimage.color import rgb2gray\n",
        "from skimage import filters\n",
        "\n",
        "# CONFIG\n",
        "DATASET_PATH = Path(\"./dataset/yalefaces\")\n",
        "\n",
        "# Test file paths. Note that the first two face is the same person,\n",
        "# and the next two face is also the same person.\n",
        "TEST_FILE_NAME = \"subject01.happy\"\n",
        "TEST_FILE_PATH = DATASET_PATH / TEST_FILE_NAME\n",
        "\n",
        "TEST_FILE_NAME_2 = \"subject01.sad\"\n",
        "TEST_FILE_PATH_2 = DATASET_PATH / TEST_FILE_NAME_2\n",
        "\n",
        "TEST_FILE_NAME_3 = \"subject04.sad\"\n",
        "TEST_FILE_PATH_3 = DATASET_PATH / TEST_FILE_NAME_3\n",
        "\n",
        "TEST_FILE_NAME_4 = \"subject04.happy\"\n",
        "TEST_FILE_PATH_4 = DATASET_PATH / TEST_FILE_NAME_4\n",
        "\n",
        "def dwt(img: np.ndarray) -> Tuple[np.ndarray]:\n",
        "    \"\"\" Perform discrete wavelet transform on the image.\n",
        "\n",
        "    Args:\n",
        "        img (np.ndarray): The matrix representation of the image.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray]: Return lowpass and highpass portions combination.\n",
        "    \"\"\"\n",
        "    coeffs2 = pywt.dwt2(img, 'bior1.3')\n",
        "    LL, (LH, HL, HH) = coeffs2\n",
        "    return LL, LH, HL, HH\n",
        "\n",
        "def show_wavelet_decomposition(img: np.ndarray) -> None:\n",
        "    \"\"\" Draw the graph of the wavelet 2-level decomposition.\n",
        "\n",
        "    Args:\n",
        "        img (np.ndarray): The numpy representation of the image.\n",
        "    \"\"\"\n",
        "    level_1_transform = list(dwt(img)) # This method will return LL, LH, HL, HH\n",
        "    level_2_transform = []\n",
        "    for f in level_1_transform:\n",
        "        # concatenate new tranformed feature images for each transformed portion\n",
        "        # in level 1 transform.\n",
        "        level_2_transform = [*level_2_transform, *list(dwt(f))]\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 12))\n",
        "    # For each feature image in the level 2 transform.\n",
        "    for i, a in enumerate(level_2_transform):\n",
        "        ax = fig.add_subplot(4, 4, i + 1)\n",
        "        ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show(block=True)\n",
        "\n",
        "def draw_horizontal_baselines(img: np.ndarray, \n",
        "                    baselines: List[int], \n",
        "                    on_img: np.ndarray=None) -> np.ndarray:\n",
        "    \"\"\" Draw the horizontal baselines on the image by setting all value to 0.\n",
        "\n",
        "    Args:\n",
        "        img (np.ndarray): shape(height, width)\n",
        "        baselines (List[int]): list of baselines (rows)\n",
        "        on_img: np.ndarray: The image that the user want to draw \n",
        "            the baseline on, if it is None, then the original image is\n",
        "            copied and then drawn baseline on that image. Default to None.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The image with the horizontal baseline.\n",
        "    \"\"\"\n",
        "    drawn = None\n",
        "    if on_img is not None:\n",
        "        assert on_img.shape == img.shape, \"The two image should have the same dimension.\"\n",
        "        drawn = on_img.copy()\n",
        "    else:\n",
        "        drawn = img.copy()\n",
        "    for baseline in baselines:\n",
        "        drawn[baseline,:] = 1\n",
        "    return drawn\n",
        "\n",
        "def draw_vertical_baselines(img: np.ndarray, \n",
        "                    baselines: List[int], \n",
        "                    on_img: np.ndarray=None) -> np.ndarray:\n",
        "    \"\"\" Draw the vertical baselines on the image by setting all value to 0.\n",
        "\n",
        "    Args:\n",
        "        img (np.ndarray): shape(height, width)\n",
        "        baselines (List[int]): list of baselines (rows)\n",
        "        on_img: np.ndarray: The image that the user want to draw \n",
        "            the baseline on, if it is None, then the original image is\n",
        "            copied and then drawn baseline on that image. Default to None.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The image with the vertical baseline.\n",
        "    \"\"\"\n",
        "    drawn = None\n",
        "    if on_img is not None:\n",
        "        assert on_img.shape == img.shape, \"The two image should have the same dimension.\"\n",
        "        drawn = on_img.copy()\n",
        "    else:\n",
        "        drawn = img.copy()\n",
        "    for baseline in baselines:\n",
        "        drawn[:,baseline] = 1\n",
        "    return drawn\n",
        "\n",
        "def draw_bounding_box(img: np.ndarray, bounding_box: Tuple[Tuple[int]]) -> np.ndarray:\n",
        "    \"\"\" Return a copy of the image with the bounding box drawn.\n",
        "\n",
        "    Args:\n",
        "        img ([type]): The numpy representation of the image.\n",
        "        bounding_box ([type]): The tuple of the points in the bounding box.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The newly copied image with the drawn bounding box.\n",
        "    \"\"\"\n",
        "    new_img = img.copy()\n",
        "    ((ulx, uly), (urx, _), (_, bly), (_, _)) = bounding_box\n",
        "    for i in range(uly, bly + 1):\n",
        "        if i == uly or i == bly:\n",
        "            new_img[i, ulx : urx + 1] = 1\n",
        "        else:\n",
        "            new_img[i, ulx] = 1\n",
        "            new_img[i, urx] = 1\n",
        "    return new_img\n",
        "\n",
        "def get_max_value_indices(values: np.ndarray, n: int=3) -> List[int]:\n",
        "    \"\"\" Get the indices of the max value in the array\n",
        "\n",
        "    Args:\n",
        "        values (np.ndarray): Shape (-1,). 1D array\n",
        "        n (int, optional): The number of indices. Defaults to 3.\n",
        "\n",
        "    Returns:\n",
        "        List[int]: The indices of n max value in the values array.\n",
        "    \"\"\"\n",
        "    return (-values).argsort()[:n]\n",
        "\n",
        "def horizontal_projection(img: np.ndarray) -> np.ndarray:\n",
        "    \"\"\" Compute the horizontal projection of an image\n",
        "\n",
        "    Args:\n",
        "        img (np.ndarray): 2d array. Shape(height, width)\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The 1d matrix representing the horizontal projection\n",
        "            of that vector.\n",
        "    \"\"\"\n",
        "    assert len(img.shape) == 2, \"Wrong behavior due to wrong image dimension.\"\n",
        "    return np.sum(img, axis=0)\n",
        "\n",
        "def vertical_projection(img: np.ndarray) -> np.ndarray:\n",
        "    \"\"\" Compute the vertical projection of an image\n",
        "\n",
        "    Args:\n",
        "        img (np.ndarray): 2d array. Shape(height, width)\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The 1d matrix representing the vertical projection\n",
        "            of that vector.\n",
        "    \"\"\"\n",
        "    assert len(img.shape) == 2, \"Wrong behavior due to wrong image dimension.\"\n",
        "    return np.sum(img, axis=1)\n",
        "\n",
        "def show_img(img: np.ndarray):\n",
        "    \"\"\" Show the image in a gray scale manner\n",
        "\n",
        "    Args:\n",
        "        img (np.ndarray): the numpy representation of the matrix.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.imshow(img, cmap=plt.cm.gray)\n",
        "    plt.show()\n",
        "\n",
        "def get_mean(img: np.ndarray) -> float:\n",
        "    return np.mean(img)\n",
        "\n",
        "def get_variance(img: np.ndarray) -> float:\n",
        "    return np.var(img)\n",
        "\n",
        "def get_center_horizontal_baselines(horizontal_baselines: List[int]) -> int:\n",
        "    horizontal_baselines.sort()\n",
        "    return horizontal_baselines[len(horizontal_baselines) // 2]\n",
        "\n",
        "def get_bounding_box(horizontal_baselines: np.ndarray, \n",
        "                    vertical_baselines: np.ndarray) -> Tuple[Tuple[int]]:\n",
        "    hb = np.sort(horizontal_baselines)\n",
        "    vb = np.sort(vertical_baselines)\n",
        "\n",
        "    ul = (vb[0], hb[0])\n",
        "    ur = (vb[-1], hb[0])\n",
        "    bl = (vb[0], hb[-1])\n",
        "    br = (vb[-1], hb[-1])\n",
        "    return ul, ur, bl, br\n",
        "\n",
        "def get_top_bottom_image(img: np.ndarray, \n",
        "                        bounding_box: Tuple[Tuple[int]], \n",
        "                        mid: int) -> Tuple[np.ndarray]:\n",
        "    \"\"\" Given an image, a bounding box, and a middle baseline, separate,\n",
        "    the bounding box section into 2 part, the top part and the bottom part.\n",
        "\n",
        "    Args:\n",
        "        img (np.ndarray): the image itself\n",
        "        bounding_box (Tuple[Tuple[int]]): the bounding box containing 4 points in the\n",
        "            image. Each point is a tuple of 2 element representing the x-index (column\n",
        "            index) and y-index (row index).\n",
        "        mid (int): the index of the middle baseline.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The numpy representation of the top bounding box portion.\n",
        "        np.ndarray: The numpy representation of the bottom bounding box portion.\n",
        "    \"\"\"\n",
        "    ((ulx, uly), (urx, _), (blx, bly), (brx, _)) = bounding_box\n",
        "    return img[uly:mid, ulx:urx], img[mid:bly, blx:brx]\n",
        "\n",
        "def get_feature_vector(img: np.ndarray, verbose: bool=False) -> np.ndarray:\n",
        "    \"\"\" Given an image, extract the feature of that image by performing\n",
        "    discrete wavelet transform, extracting mean and variancce from those\n",
        "    and from the top and bottom bounding box portion. Each tuple of mean \n",
        "    and variance is considered as a feature in a feature vector.\n",
        "\n",
        "    Args:\n",
        "        img (np.ndarray): The numpy representation of the image.\n",
        "        verbose (bool): Whether to print the output or not. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The numpy representation of the feature vector.\n",
        "            Shape: (17, 2). The 15 first features is the (mean, variance) from\n",
        "            15 feature images that was wavelet decomposited. The last 2 features\n",
        "            is the (mean, variance) of the top and bottom portion in the bounding\n",
        "            box.\n",
        "    \"\"\"\n",
        "    # Define the feature vector matrix\n",
        "    feature_vector: List[List[float]] = []\n",
        "\n",
        "    # For the first type of features (15 features of tuple containing mean and var)\n",
        "    # Do the 2 level dwt.\n",
        "    # Extract mean and variance of the 2-level of 15 feature imgs, but not the\n",
        "    # low pass one.\n",
        "    # Append to the feature vector.\n",
        "    level_1_transform = list(dwt(img)) # This method will return LL, LH, HL, HH\n",
        "    level_2_transform = []\n",
        "    for f in level_1_transform:\n",
        "        # concatenate new tranformed feature images for each transformed portion\n",
        "        # in level 1 transform.\n",
        "        level_2_transform = [*level_2_transform, *list(dwt(f))]\n",
        "\n",
        "    # For each feature image in the level 2 transform.\n",
        "    for i, data in enumerate(level_2_transform):\n",
        "        # If it's not the lowpass of the lowpass in level 2, compute mean and variance,\n",
        "        # and add it to the feature vector.\n",
        "        if i > 0:\n",
        "            feature_vector.append([get_mean(data), get_variance(data)])\n",
        "\n",
        "    # For the second type of feature (3 feature of tuple containing mean an var)\n",
        "    # Get the mean and var of the upper-nose part and the bottom-nose part.\n",
        "    # Append to the feature vector. In this case we use sobel filter to detech edge.\n",
        "\n",
        "    # Get vertical baselines\n",
        "    edges = filters.sobel(img)\n",
        "\n",
        "    # Get the middle horizontal line index of the image.\n",
        "    mid = edges.shape[0] // 2\n",
        "\n",
        "    # Compute the left and right verticle lines that bound the face.\n",
        "    left_bound_id = -1\n",
        "    for i, data in enumerate(edges[mid]):\n",
        "        if data > 0:\n",
        "            left_bound_id = i\n",
        "            break\n",
        "    right_bound_id = -1\n",
        "    for i in range(len(edges[0]) - 1, -1, -1):\n",
        "        if edges[mid, i]:\n",
        "            right_bound_id = i\n",
        "            break\n",
        "\n",
        "    if verbose:\n",
        "        # Print information\n",
        "        print(\"Vertical baselines: \" + str([left_bound_id, right_bound_id]))\n",
        "        vertical_baseline_img = draw_vertical_baselines(edges, [left_bound_id, right_bound_id], img)\n",
        "        show_img(vertical_baseline_img)\n",
        "\n",
        "    # Compute the index of horizontal baselines\n",
        "    vp = vertical_projection(edges[3:-3, :]) # Since the projection in the border of the\n",
        "                                                # image is noisy and trivial, crop those parts\n",
        "                                                # and perform vertical projection on the cropped\n",
        "                                                # to get the sum of each horizontal line.\n",
        "    # Initialize horizontal baselines index vector.\n",
        "    horizontal_baselines = []\n",
        "\n",
        "    # What happen next is magic!\n",
        "    # This kind of data structure is a collection that only contains unique value.\n",
        "    s = set()\n",
        "\n",
        "    # Number of maximum index that we want to get\n",
        "    n = 6\n",
        "\n",
        "    # the range in the image the the lines have to be far away from each other.\n",
        "    # I.e, each line index should be more than 20 lines far away from each other.\n",
        "    range_threshold = 20\n",
        "\n",
        "    # The sorted indices in the vertical projection (largest to smaller).\n",
        "    # This sorted indices are also the line horizontal line index in the image.\n",
        "    sorted_idx = (-vp).argsort()\n",
        "\n",
        "    # traverse index in the soreted idx vector.\n",
        "    i = 0 \n",
        "\n",
        "    # For each line starting from the largest, if we still want to get more index and\n",
        "    # we have not reached the end of the sorted idx matrix (the number of vertical projection):\n",
        "    while n > 0 and i < len(sorted_idx):\n",
        "        # If the line is not in the set, then process\n",
        "        if sorted_idx[i] not in s:\n",
        "            # Append it to the horizontal baseline.\n",
        "            horizontal_baselines.append(sorted_idx[i])\n",
        "            \n",
        "            # Also add that to the set so next time we don't add this line to the horizontal \n",
        "            # baseline vector.\n",
        "            s.add(sorted_idx[i])\n",
        "\n",
        "            # For each line in 20 line up and 20 line down, we also add it to the set\n",
        "            # So next time we don't add any of those to the horizontal baseline vector.\n",
        "            for k in range(range_threshold):\n",
        "                s.add(sorted_idx[i] - k)\n",
        "                s.add(sorted_idx[i] + k)\n",
        "            n -= 1\n",
        "        i += 1\n",
        "\n",
        "    if verbose:\n",
        "        # Print information\n",
        "        print(\"Horizontal baselines: \" + str(horizontal_baselines))\n",
        "        horizontal_baselines_img = draw_horizontal_baselines(edges, horizontal_baselines, img)\n",
        "        show_img(horizontal_baselines_img)\n",
        "        print(\"Every baseline:\")\n",
        "        total_baseline_img = draw_horizontal_baselines(edges, horizontal_baselines, vertical_baseline_img)\n",
        "        show_img(total_baseline_img)\n",
        "\n",
        "    # Get the bounding box\n",
        "    bounding_box = get_bounding_box(np.asarray(horizontal_baselines), \n",
        "                                    [left_bound_id, right_bound_id])\n",
        "\n",
        "    if verbose:\n",
        "        # Print the bounding box\n",
        "        print(\"Draw bounding box\")\n",
        "        bdb = draw_bounding_box(img, bounding_box)\n",
        "        show_img(bdb)\n",
        "\n",
        "    # Get the middle horizontal baseline\n",
        "    mid_baseline = get_center_horizontal_baselines(horizontal_baselines)\n",
        "\n",
        "    # Get the seperated top and bottom image.\n",
        "    top_image, bottom_img = get_top_bottom_image(img, bounding_box, mid_baseline)\n",
        "\n",
        "    # Finally append the two features (mean and variance of the top and bottom bounding\n",
        "    # box portions) to the feature vector.\n",
        "    feature_vector.append([get_mean(top_image), get_variance(top_image)])\n",
        "    feature_vector.append([get_mean(bottom_img), get_variance(bottom_img)])\n",
        "\n",
        "    if verbose:\n",
        "        # Print feature vector\n",
        "        print(\"Feature vector:\")\n",
        "        print(np.asarray(feature_vector))\n",
        "        print()\n",
        "    return np.asarray(feature_vector)\n",
        "\n",
        "\n",
        "def get_B_distance(f1: List[float], f2: List[float]) -> float:\n",
        "    \"\"\" The idea of the Bhattacharrya distance is referred from this paper:\n",
        "    https://www.researchgate.net/publication/222546775_Wavelet_Packet_Analysis_for_Face_Recognition\n",
        "\n",
        "    Args:\n",
        "        f1 (List[float]): a particular feature with two value [mean, variance]\n",
        "        f1 (List[float]): a particular feature with two value [mean, variance]\n",
        "\n",
        "    Returns:\n",
        "        float: a scalar (number) representation of the distance between 2 features.\n",
        "    \"\"\"\n",
        "    [mean1, var1] = f1\n",
        "    [mean2, var2] = f2\n",
        "    return (1 / 4) * (((mean1 - mean2) ** 2) / (var1 + var2)) + \\\n",
        "        (1 / 2) * np.log(((1 / 2) * (var1 + var2)) / math.sqrt(var1 * var2))\n",
        "\n",
        "def get_feature_distancce(v1: np.ndarray, v2: np.ndarray) -> float:\n",
        "    \"\"\" The idea of computing the distance between 2 image is computed by the sum of the\n",
        "    distance among each corresponsing feature of each image's feature vector.\n",
        "    The idea is referred from this paper:\n",
        "    https://www.researchgate.net/publication/222546775_Wavelet_Packet_Analysis_for_Face_Recognition\n",
        "\n",
        "    Args:\n",
        "        v1 (np.ndarray): an image's feature vector. Shape: (17, 2)\n",
        "        v2 (np.ndarray): an image's feature vector. Shape: (17, 2)\n",
        "\n",
        "    Returns:\n",
        "        (float): The distance between 2 vectors.\n",
        "    \"\"\"\n",
        "    s: float = 0\n",
        "    for f1, f2 in zip(v1, v2):\n",
        "        s += get_B_distance(f1, f2)\n",
        "    return s\n",
        "\n",
        "def discrete_comparison(v1: np.ndarray, v2: np.ndarray, distance_threshold: float=0.1) -> bool:\n",
        "    \"\"\" Determine if the image is the same person or not.\n",
        "\n",
        "    Args:\n",
        "        v1 (np.ndarray): an image's feature vector. Shape: (17, 2)\n",
        "        v2 (np.ndarray): an image's feature vector. Shape: (17, 2)\n",
        "        distance_threshold (float, optional): The distance threshold. Defaults to 0.1.\n",
        "\n",
        "    Returns:\n",
        "        bool: Return True if the distance of 2 feature vectors is smaller than distance_threshold,\n",
        "            False otherwise.\n",
        "    \"\"\"\n",
        "    feature_distance = get_feature_distancce(v1, v2)\n",
        "    return feature_distance <= distance_threshold\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkkrxCb4a62N"
      },
      "source": [
        "################## EXPERIMENT! ####################\n",
        "\n",
        "# Here is the eample of the wavelet transform\n",
        "\n",
        "# Load image\n",
        "original = pywt.data.camera()\n",
        "print(\"Original image:\")\n",
        "show_img(original)\n",
        "# Wavelet transform of image, and plot approximation and details\n",
        "titles = ['Approximation', ' Horizontal detail',\n",
        "          'Vertical detail', 'Diagonal detail']\n",
        "coeffs2 = pywt.dwt2(original, 'bior1.3')\n",
        "LL, (LH, HL, HH) = coeffs2\n",
        "print(\"Discrete wavelet transform portions:\")\n",
        "fig = plt.figure(figsize=(12, 3))\n",
        "for i, a in enumerate([LL, LH, HL, HH]):\n",
        "    ax = fig.add_subplot(1, 4, i + 1)\n",
        "    ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n",
        "    ax.set_title(titles[i], fontsize=10)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmP2dogFa62O"
      },
      "source": [
        "\n",
        "# Load the 4 images\n",
        "img1 = np.asarray(Image.open(TEST_FILE_PATH))\n",
        "img2 = np.asarray(Image.open(TEST_FILE_PATH_2))\n",
        "img3 = np.asarray(Image.open(TEST_FILE_PATH_3))\n",
        "img4 = np.asarray(Image.open(TEST_FILE_PATH_4))\n",
        "\n",
        "# Show 4 test image\n",
        "show_img(img1)\n",
        "show_img(img2)\n",
        "show_img(img3)\n",
        "show_img(img4)\n",
        "\n",
        "print(\"Wavelet packet decomposition of image 1: \")\n",
        "show_wavelet_decomposition(img1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJtM12Gra62P"
      },
      "source": [
        "\n",
        "# Read image \n",
        "img: np.ndarray = np.asarray(Image.open(TEST_FILE_PATH))\n",
        "print(\"original image:\")\n",
        "show_img(img)\n",
        "LL, LH, HL, HH = dwt(img)\n",
        "# show_img(HH)\n",
        "LL1, LH1, HL1, HH1 = dwt(LL)\n",
        "# show_img(LH1)\n",
        "print(\"Image with baseline:\")\n",
        "vp = vertical_projection(HH1)\n",
        "horizontal_baselines = get_max_value_indices(vp, 4)\n",
        "new_LL1 = draw_horizontal_baselines(HH1, horizontal_baselines, LL1)\n",
        "show_img(new_LL1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD0K8cQea62P"
      },
      "source": [
        "\n",
        "im = img# RGB image to gray scale\n",
        "plt.gray()\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.subplot(221)\n",
        "plt.imshow(im)\n",
        "plt.title('original', size=20)\n",
        "plt.subplot(222)\n",
        "edges_y = filters.sobel_h(im) \n",
        "plt.imshow(edges_y)\n",
        "plt.title('sobel_x', size=20)\n",
        "plt.subplot(223)\n",
        "edges_x = filters.sobel_v(im)\n",
        "plt.imshow(edges_x)\n",
        "plt.title('sobel_y', size=20)\n",
        "plt.subplot(224)\n",
        "edges = filters.sobel(im)\n",
        "plt.imshow(edges)\n",
        "plt.title('sobel', size=20)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZFTzd8Ya62Q"
      },
      "source": [
        "\n",
        "hp = horizontal_projection(edges)\n",
        "\n",
        "mid = edges.shape[0] // 2\n",
        "left_bound_id = -1\n",
        "for i, data in enumerate(edges[mid]):\n",
        "    if data > 0:\n",
        "        left_bound_id = i\n",
        "        break\n",
        "\n",
        "right_bound_id = -1\n",
        "for i in range(len(edges[0]) - 1, -1, -1):\n",
        "    if edges[mid, i]:\n",
        "        right_bound_id = i\n",
        "        break\n",
        "\n",
        "new_edges = draw_vertical_baselines(edges, [left_bound_id, right_bound_id], img)\n",
        "show_img(new_edges)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KVnAUtLa62Q"
      },
      "source": [
        "\n",
        "vp = vertical_projection(edges[3:-3, :])\n",
        "\n",
        "horizontal_baselines = []\n",
        "s = set()\n",
        "n = 6\n",
        "range_threshold = 20\n",
        "i = 0 # traverse index in the soreted idx\n",
        "sorted_idx = (-vp).argsort()\n",
        "while n > 0 and i < len(sorted_idx):\n",
        "    if sorted_idx[i] not in s:\n",
        "        horizontal_baselines.append(sorted_idx[i])\n",
        "        s.add(sorted_idx[i])\n",
        "        for k in range(range_threshold):\n",
        "            s.add(sorted_idx[i] - k)\n",
        "            s.add(sorted_idx[i] + k)\n",
        "        n -= 1\n",
        "    i += 1\n",
        "\n",
        "new_edges = draw_horizontal_baselines(edges, horizontal_baselines, new_edges)\n",
        "show_img(new_edges)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfeVtZama62Q"
      },
      "source": [
        "\n",
        "# bounding box can be represented as ((ulx, uly), (urx, ury), (blx, bly), (brx, bry))\n",
        "bounding_box = get_bounding_box(np.asarray(horizontal_baselines), [left_bound_id, right_bound_id])\n",
        "\n",
        "print(\"Image with bounding box\")\n",
        "img_bounding = draw_bounding_box(img, bounding_box)\n",
        "show_img(img_bounding)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU30s-hua62R"
      },
      "source": [
        "\n",
        "# We have seen the overview of the image works. Now is the time to do the actual feature\n",
        "# extraction and comparison.\n",
        "\n",
        "img = np.asarray(Image.open(TEST_FILE_PATH_2))\n",
        "img2 = np.asarray(Image.open(TEST_FILE_PATH_3))\n",
        "\n",
        "# Compare between 2 image.\n",
        "print(\"Comparing these 2 images:\")\n",
        "fig = plt.figure()\n",
        "a = fig.add_subplot(1, 2, 1)\n",
        "a.set_xticks([])\n",
        "a.set_yticks([])\n",
        "a.imshow(img, cmap=plt.cm.gray)\n",
        "a = fig.add_subplot(1, 2, 2)\n",
        "a.imshow(img2, cmap=plt.cm.gray)\n",
        "a.set_xticks([])\n",
        "a.set_yticks([])\n",
        "plt.show()\n",
        "\n",
        "# You can change the verbose to False to not printing the processed images in the method.\n",
        "feature_vector = get_feature_vector(img, verbose=True)\n",
        "\n",
        "# You can change the verbose to False to not printing the processed images in the method.\n",
        "feature_vector_2 = get_feature_vector(img2, verbose=True)\n",
        "\n",
        "# Get distance\n",
        "s = get_feature_distancce(feature_vector, feature_vector_2)\n",
        "\n",
        "print(\"Computed distance: \" + str(s))\n",
        "\n",
        "same_person = discrete_comparison(feature_vector, feature_vector_2)\n",
        "if same_person:\n",
        "    print(\"The two images are the same person.\")\n",
        "else:\n",
        "    print(\"The two images are the different person.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC5rCrBKa62R"
      },
      "source": [
        "\n",
        "# Now compare with the same person but different feeling.\n",
        "img = np.asarray(Image.open(TEST_FILE_PATH))\n",
        "img2 = np.asarray(Image.open(TEST_FILE_PATH_2))\n",
        "\n",
        "# Compare between 2 image.\n",
        "print(\"Comparing these 2 images:\")\n",
        "fig = plt.figure()\n",
        "a = fig.add_subplot(1, 2, 1)\n",
        "a.set_xticks([])\n",
        "a.set_yticks([])\n",
        "a.imshow(img, cmap=plt.cm.gray)\n",
        "a = fig.add_subplot(1, 2, 2)\n",
        "a.imshow(img2, cmap=plt.cm.gray)\n",
        "a.set_xticks([])\n",
        "a.set_yticks([])\n",
        "plt.show()\n",
        "\n",
        "# You can change the verbose to False to not printing the processed images in the method.\n",
        "feature_vector = get_feature_vector(img, verbose=True)\n",
        "\n",
        "# You can change the verbose to False to not printing the processed images in the method.\n",
        "feature_vector_2 = get_feature_vector(img2, verbose=True)\n",
        "\n",
        "# Get distance\n",
        "s = get_feature_distancce(feature_vector, feature_vector_2)\n",
        "\n",
        "print(\"Computed distance: \" + str(s))\n",
        "\n",
        "same_person = discrete_comparison(feature_vector, feature_vector_2)\n",
        "if same_person:\n",
        "    print(\"The two images are the same person.\")\n",
        "else:\n",
        "    print(\"The two images are the different person.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH7FFi86a62R"
      },
      "source": [
        "\n",
        "# Now to mass comparison.\n",
        "\n",
        "def compare_image(img1: np.ndarray, img2: np.ndarray) -> None:\n",
        "    \"\"\" Compare image with no verbose in order to mass comparing\n",
        "\n",
        "    Args:\n",
        "        img1 (np.ndarray): the numpy representation of the first image\n",
        "        img2 (np.ndarray): the numpy representation of the first image\n",
        "    \"\"\"\n",
        "    print(\"Comparing these 2 images:\")\n",
        "    fig = plt.figure()\n",
        "    a = fig.add_subplot(1, 2, 1)\n",
        "    a.set_xticks([])\n",
        "    a.set_yticks([])\n",
        "    a.imshow(img1, cmap=plt.cm.gray)\n",
        "    a = fig.add_subplot(1, 2, 2)\n",
        "    a.imshow(img2, cmap=plt.cm.gray)\n",
        "    a.set_xticks([])\n",
        "    a.set_yticks([])\n",
        "    plt.show()\n",
        "\n",
        "    # You can change the verbose to False to not printing the processed images in the method.\n",
        "    feature_vector = get_feature_vector(img1, verbose=False)\n",
        "\n",
        "    # You can change the verbose to False to not printing the processed images in the method.\n",
        "    feature_vector_2 = get_feature_vector(img2, verbose=False)\n",
        "\n",
        "    # Get distance\n",
        "    s = get_feature_distancce(feature_vector, feature_vector_2)\n",
        "\n",
        "    print(\"Computed distance: \" + str(s))\n",
        "\n",
        "    same_person = discrete_comparison(feature_vector, feature_vector_2)\n",
        "    if same_person:\n",
        "        print(\"The two images are the same person.\")\n",
        "    else:\n",
        "        print(\"The two images are the different person.\")\n",
        "    print()\n",
        "\n",
        "# Mass comparing\n",
        "# There are some distinct status of each face\n",
        "status = ['glasses', 'happy', 'sad', 'sleepy', 'wink', 'leftlight', 'noglasses']\n",
        "# There are also 15 total different people in the database.\n",
        "n_subjects = 15\n",
        "batch = 20\n",
        "for i in range(batch):\n",
        "    r1 = int(np.random.randint(0, len(status) - 1))\n",
        "    r2 = int(np.random.randint(0, len(status) - 1))\n",
        "    s1 = np.random.randint(1, n_subjects)\n",
        "    s1 = \"0\" + str(s1) if s1 < 10 else str(s1)\n",
        "    s2 = np.random.randint(1, n_subjects)\n",
        "    s2 = \"0\" + str(s2) if s2 < 10 else str(s2)\n",
        "    path1 = DATASET_PATH / f\"subject{s1}.{status[r1]}\"\n",
        "    path2 = DATASET_PATH / f\"subject{s2}.{status[r2]}\"\n",
        "    try:\n",
        "        img1 = np.asarray(Image.open(path1))\n",
        "        img2 = np.asarray(Image.open(path2))\n",
        "        compare_image(img1, img2)\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found, next subject!\")\n",
        "    except:\n",
        "        print(\"Unexpected error:\", sys.exc_info()[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}