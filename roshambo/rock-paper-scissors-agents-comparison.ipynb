{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Rock Paper Scissors - Legion Of Agents - Arena\n\nThis notebook contains a lot of different agents from different sources.    \nIn the **Comparison In Battle** section, we also added a comparison in a fair battle of each agent with each in 1 round with 1000 steps."},{"metadata":{},"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-competitions/kaggle/22838/logos/header.png?t=2020-11-02-21-55-44)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:black; background:#FBE338; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation</center></h3>\n\n* [Agent: Hit The Last Own Action](#1)\n* [Agent: Rock](#2)\n* [Agent: Paper](#3)\n* [Agent: Scissors](#4)\n* [Agent: Copy Opponent](#5)\n* [Agent: Reactionary](#6)\n* [Agent: Counter Reactionary](#7)\n* [Agent: Statistical](#8)\n* [Agent: Nash Equilibrium](#9)\n* [Agent: Markov Agent](#10)\n* [Agent: Memory Patterns](#11)\n* [Agent: Multi Armed Bandit](#12)\n* [Agent: Opponent Transition Matrix](#13)\n* [Agent: Decision Tree Classifier](#14)\n\n    \n* [Example Using The kaggle_environments For Testing Agents](#100)\n* [Comparison In Battle](#101)\n* [Test Your Own Agent](#102)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Hit The Last Own Action<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"The idea of the agent:\n\n- A lot of agents use a simple baseline - copy the last action of the opponent.   \n- That's why we can simply hit our last actions (new action of the opponent)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile hit_the_last_own_action.py\n\nmy_last_action = 0\n\ndef hit_the_last_own_action(observation, configuration):\n    global my_last_action\n    my_last_action = (my_last_action + 1) % 3\n    \n    return my_last_action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Rock<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"Copy from: https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/rps/agents.py    \n\nAlways uses Rock action"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile rock.py\n\ndef rock(observation, configuration):\n    return 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Paper<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"Copy from: https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/rps/agents.py\n\nAlways uses Paper action"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile paper.py\n\ndef paper(observation, configuration):\n    return 1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Scissors<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"Copy from: https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/rps/agents.py\n\nAlways uses Scissors action"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile scissors.py\n\ndef scissors(observation, configuration):\n    return 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Copy Opponent<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"Copy from: https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/rps/agents.py"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile copy_opponent.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\ndef copy_opponent(observation, configuration):\n    if observation.step > 0:\n        return observation.lastOpponentAction\n    else:\n        return random.randrange(0, configuration.signs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Reactionary<center><h2>\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from: https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/rps/agents.py"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_react_action = None\n\n\ndef reactionary(observation, configuration):\n    global last_react_action\n    if observation.step == 0:\n        last_react_action = random.randrange(0, configuration.signs)\n    elif get_score(last_react_action, observation.lastOpponentAction) <= 1:\n        last_react_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_react_action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Counter Reactionary<center><h2>\n\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from: https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/rps/agents.py"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile counter_reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_counter_action = None\n\n\ndef counter_reactionary(observation, configuration):\n    global last_counter_action\n    if observation.step == 0:\n        last_counter_action = random.randrange(0, configuration.signs)\n    elif get_score(last_counter_action, observation.lastOpponentAction) == 1:\n        last_counter_action = (last_counter_action + 2) % configuration.signs\n    else:\n        last_counter_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_counter_action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Statistical<center><h2>\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from: https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/rps/agents.py"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile statistical.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\naction_histogram = {}\n\ndef statistical(observation, configuration):\n    global action_histogram\n    if observation.step == 0:\n        action_histogram = {}\n        return\n    action = observation.lastOpponentAction\n    if action not in action_histogram:\n        action_histogram[action] = 0\n    action_histogram[action] += 1\n    mode_action = None\n    mode_action_count = None\n    for k, v in action_histogram.items():\n        if mode_action_count is None or v > mode_action_count:\n            mode_action = k\n            mode_action_count = v\n            continue\n\n    return (mode_action + 1) % configuration.signs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Nash Equilibrium<center><h2>\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from kernel [Rock Paper Scissors - Nash Equilibrium Strategy](https://www.kaggle.com/ihelon/rock-paper-scissors-nash-equilibrium-strategy)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile nash_equilibrium.py\n\nimport random\n\ndef nash_equilibrium(observation, configuration):\n    return random.randint(0, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Markov Agent<center><h2>\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from kernel [(Not so) Markov ⛓️](https://www.kaggle.com/alexandersamarin/not-so-markov)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile markov_agent.py\n\nimport numpy as np\nimport collections\n\ndef markov_agent(observation, configuration):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0: # refresh table every 250 steps\n        action_seq, table = [], collections.defaultdict(lambda: [1, 1, 1])    \n    if len(action_seq) <= 2 * k + 1:\n        action = int(np.random.randint(3))\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = np.argmax(table[key])\n    else:\n        scores = np.array(table[key])\n        next_opponent_action_pred = np.random.choice(3, p=scores/scores.sum()) # add stochasticity for second part of the game\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return int(action)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Memory Patterns<center><h2>\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from kernel [Rock, Paper, Scissors with Memory Patterns](https://www.kaggle.com/yegorbiryukov/rock-paper-scissors-with-memory-patterns)"},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%writefile memory_patterns.py\n\nimport random\n\n# how many steps in a row are in the pattern (multiplied by two)\nmemory_length = 6\n# current memory of the agent\ncurrent_memory = []\n# list of memory patterns\nmemory_patterns = []\n\ndef find_pattern(memory):\n    \"\"\" find appropriate pattern in memory \"\"\"\n    for pattern in memory_patterns:\n        actions_matched = 0\n        for i in range(memory_length):\n            if pattern[\"actions\"][i] == memory[i]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return pattern\n    # appropriate pattern not found\n    return None\n\ndef my_agent(obs, conf):\n    \"\"\" your ad here \"\"\"\n    # if it's not first step, add opponent's last action to agent's current memory\n    if obs[\"step\"] > 0:\n        current_memory.append(obs[\"lastOpponentAction\"])\n    # if length of current memory is bigger than necessary for a new memory pattern\n    if len(current_memory) > memory_length:\n        # get momory of the previous step\n        previous_step_memory = current_memory[:memory_length]\n        previous_pattern = find_pattern(previous_step_memory)\n        if previous_pattern == None:\n            previous_pattern = {\n                \"actions\": previous_step_memory.copy(),\n                \"opp_next_actions\": [\n                    {\"action\": 0, \"amount\": 0, \"response\": 1},\n                    {\"action\": 1, \"amount\": 0, \"response\": 2},\n                    {\"action\": 2, \"amount\": 0, \"response\": 0}\n                ]\n            }\n            memory_patterns.append(previous_pattern)\n        for action in previous_pattern[\"opp_next_actions\"]:\n            if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                action[\"amount\"] += 1\n        # delete first two elements in current memory (actions of the oldest step in current memory)\n        del current_memory[:2]\n    my_action = random.randint(0, 2)\n    pattern = find_pattern(current_memory)\n    if pattern != None:\n        my_action_amount = 0\n        for action in pattern[\"opp_next_actions\"]:\n            # if this opponent's action occurred more times than currently chosen action\n            # or, if it occured the same amount of times, choose action randomly among them\n            if (action[\"amount\"] > my_action_amount or\n                    (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                my_action_amount = action[\"amount\"]\n                my_action = action[\"response\"]\n    current_memory.append(my_action)\n    return my_action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Multi Armed Bandit<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"Copy from kernel [Multi-armed bandit vs deterministic agents](https://www.kaggle.com/ilialar/multi-armed-bandit-vs-deterministic-agents)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%writefile multi_armed_bandit.py\n\n\nimport pandas as pd\nimport numpy as np\nimport json\n\n\n# base class for all agents, random agent\nclass agent():\n    def initial_step(self):\n        return np.random.randint(3)\n    \n    def history_step(self, history):\n        return np.random.randint(3)\n    \n    def step(self, history):\n        if len(history) == 0:\n            return int(self.initial_step())\n        else:\n            return int(self.history_step(history))\n    \n# agent that returns (previousCompetitorStep + shift) % 3\nclass mirror_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['competitorStep'] + self.shift) % 3\n    \n    \n# agent that returns (previousPlayerStep + shift) % 3\nclass self_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['step'] + self.shift) % 3    \n\n\n# agent that beats the most popular step of competitor\nclass popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['competitorStep'] for x in history])\n        return (int(np.argmax(counts)) + 1) % 3\n\n    \n# agent that beats the agent that beats the most popular step of competitor\nclass anti_popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['step'] for x in history])\n        return (int(np.argmax(counts)) + 2) % 3\n    \n    \n# simple transition matrix: previous step -> next step\nclass transition_matrix(agent):\n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3)) + self.init_value\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) / self.decay + self.init_value\n            matrix[int(history[i][self.step_type]), int(history[i+1][self.step_type])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type])]/matrix[int(history[-1][self.step_type])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n    \n\n# similar to the transition matrix but rely on both previous steps\nclass transition_tensor(agent):\n    \n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type1 = 'step' \n            self.step_type2 = 'competitorStep'\n        else:\n            self.step_type2 = 'step' \n            self.step_type1 = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3, 3)) + 0.1\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) / self.decay + self.init_value\n            matrix[int(history[i][self.step_type1]), int(history[i][self.step_type2]), int(history[i+1][self.step_type1])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])]/matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n\n    \nagents = {\n    'mirror_0': mirror_shift(0),\n    'mirror_1': mirror_shift(1),  \n    'mirror_2': mirror_shift(2),\n    'self_0': self_shift(0),\n    'self_1': self_shift(1),  \n    'self_2': self_shift(2),\n    'popular_beater': popular_beater(),\n    'anti_popular_beater': anti_popular_beater(),\n    'random_transitison_matrix': transition_matrix(False, False),\n    'determenistic_transitison_matrix': transition_matrix(True, False),\n    'random_self_trans_matrix': transition_matrix(False, True),\n    'determenistic_self_trans_matrix': transition_matrix(True, True),\n    'random_transitison_tensor': transition_tensor(False, False),\n    'determenistic_transitison_tensor': transition_tensor(True, False),\n    'random_self_trans_tensor': transition_tensor(False, True),\n    'determenistic_self_trans_tensor': transition_tensor(True, True),\n    \n    'random_transitison_matrix_decay': transition_matrix(False, False, decay = 1.05),\n    'random_self_trans_matrix_decay': transition_matrix(False, True, decay = 1.05),\n    'random_transitison_tensor_decay': transition_tensor(False, False, decay = 1.05),\n    'random_self_trans_tensor_decay': transition_tensor(False, True, decay = 1.05),\n    \n    'determenistic_transitison_matrix_decay': transition_matrix(True, False, decay = 1.05),\n    'determenistic_self_trans_matrix_decay': transition_matrix(True, True, decay = 1.05),\n    'determenistic_transitison_tensor_decay': transition_tensor(True, False, decay = 1.05),\n    'determenistic_self_trans_tensor_decay': transition_tensor(True, True, decay = 1.05),\n}\n\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    \n    # bandits' params\n    step_size = 2 # how much we increase a and b \n    decay_rate = 1.05 # how much do we decay old historical data\n    \n    # I don't see how to use any global variables, so will save everything to a CSV file\n    # Using pandas for this is too much, but it can be useful later and it is convinient to analyze\n    def save_history(history, file = 'history.csv'):\n        pd.DataFrame(history).to_csv(file, index = False)\n\n    def load_history(file = 'history.csv'):\n        return pd.read_csv(file).to_dict('records')\n    \n    \n    def log_step(step = None, history = None, agent = None, competitorStep = None):\n        if step is None:\n            step = np.random.randint(3)\n        if history is None:\n            history = []\n        history.append({'step': step, 'competitorStep': competitorStep, 'agent': agent})\n        save_history(history)\n        return step\n    \n    def update_competitor_step(history, competitorStep):\n        history[-1]['competitorStep'] = int(competitorStep)\n        return history\n        \n    \n    # load history\n    if observation.step == 0:\n        history = []\n        bandit_state = {k:[1,1] for k in agents.keys()}\n    else:\n        history = update_competitor_step(load_history(), observation.lastOpponentAction)\n        \n        # load the state of the bandit\n        with open('bandit.json') as json_file:\n            bandit_state = json.load(json_file)\n        \n        # updating bandit_state using the result of the previous step\n        # we can update all states even those that were not used\n        for name, agent in agents.items():\n            agent_step = agent.step(history[:-1])\n            bandit_state[name][1] = (bandit_state[name][1] - 1) / decay_rate + 1\n            bandit_state[name][0] = (bandit_state[name][0] - 1) / decay_rate + 1\n            \n            if (history[-1]['competitorStep'] - agent_step) % 3 == 1:\n                bandit_state[name][1] += step_size\n            elif (history[-1]['competitorStep'] - agent_step) % 3 == 2:\n                bandit_state[name][0] += step_size\n            else:\n                bandit_state[name][0] += step_size/2\n                bandit_state[name][1] += step_size/2\n            \n    with open('bandit.json', 'w') as outfile:\n        json.dump(bandit_state, outfile)\n    \n    \n    # generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in bandit_state.keys():\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    step = agents[best_agent].step(history)\n    \n    return log_step(step, history, best_agent)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"13\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Opponent Transition Matrix<center><h2>\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from kernel [RPS: Opponent Transition Matrix](https://www.kaggle.com/group16/rps-opponent-transition-matrix)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%writefile opponent_transition_matrix.py\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\n# a1 is the action of the opponent 1 step ago\n# a2 is the action of the opponent 2 steps ago\na1, a2 = None, None\n\ndef transition_agent(observation, configuration):\n    global T, P, a1, a2\n    if observation.step > 1:\n        a1 = observation.lastOpponentAction\n        T[a2, a1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        a2 = a1\n        if np.sum(P[a1, :]) == 1:\n            return int((np.random.choice(\n                [0, 1, 2],\n                p=P[a1, :]\n            ) + 1) % 3)\n        else:\n            return int(np.random.randint(3))\n    else:\n        if observation.step == 1:\n            a2 = observation.lastOpponentAction\n        return int(np.random.randint(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"14\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Decision Tree Classifier<center><h2>\n\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from kernel [Decision Tree Classifier](https://www.kaggle.com/alexandersamarin/decision-tree-classifier?scriptVersionId=46415861)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%writefile decision_tree_classifier.py\n\nimport numpy as np\nimport collections\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef construct_local_features(rollouts):\n    features = np.array([[step % k for step in rollouts['steps']] for k in (2, 3, 5)])\n    features = np.append(features, rollouts['steps'])\n    features = np.append(features, rollouts['actions'])\n    features = np.append(features, rollouts['opp-actions'])\n    return features\n\ndef construct_global_features(rollouts):\n    features = []\n    for key in ['actions', 'opp-actions']:\n        for i in range(3):\n            actions_count = np.mean([r == i for r in rollouts[key]])\n            features.append(actions_count)\n    \n    return np.array(features)\n\ndef construct_features(short_stat_rollouts, long_stat_rollouts):\n    lf = construct_local_features(short_stat_rollouts)\n    gf = construct_global_features(long_stat_rollouts)\n    features = np.concatenate([lf, gf])\n    return features\n\ndef predict_opponent_move(train_data, test_sample):\n    classifier = DecisionTreeClassifier(random_state=42)\n    classifier.fit(train_data['x'], train_data['y'])\n    return classifier.predict(test_sample)\n\ndef update_rollouts_hist(rollouts_hist, last_move, opp_last_action):\n    rollouts_hist['steps'].append(last_move['step'])\n    rollouts_hist['actions'].append(last_move['action'])\n    rollouts_hist['opp-actions'].append(opp_last_action)\n    return rollouts_hist\n\ndef warmup_strategy(observation, configuration):\n    global rollouts_hist, last_move\n    action = int(np.random.randint(3))\n    if observation.step == 0:\n        last_move = {'step': 0, 'action': action}\n        rollouts_hist = {'steps': [], 'actions': [], 'opp-actions': []}\n    else:\n        rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n        last_move = {'step': observation.step, 'action': action}\n    return int(action)\n\ndef init_training_data(rollouts_hist, k):\n    for i in range(len(rollouts_hist['steps']) - k + 1):\n        short_stat_rollouts = {key: rollouts_hist[key][i:i+k] for key in rollouts_hist}\n        long_stat_rollouts = {key: rollouts_hist[key][:i+k] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, long_stat_rollouts)        \n        data['x'].append(features)\n    test_sample = data['x'][-1].reshape(1, -1)\n    data['x'] = data['x'][:-1]\n    data['y'] = rollouts_hist['opp-actions'][k:]\n    return data, test_sample\n\ndef agent(observation, configuration):\n    # hyperparameters\n    k = 5\n    min_samples = 25\n    global rollouts_hist, last_move, data, test_sample\n    if observation.step == 0:\n        data = {'x': [], 'y': []}\n    # if not enough data -> randomize\n    if observation.step <= min_samples + k:\n        return warmup_strategy(observation, configuration)\n    # update statistics\n    rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n    # update training data\n    if len(data['x']) == 0:\n        data, test_sample = init_training_data(rollouts_hist, k)\n    else:        \n        short_stat_rollouts = {key: rollouts_hist[key][-k:] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, rollouts_hist)\n        data['x'].append(test_sample[0])\n        data['y'] = rollouts_hist['opp-actions'][k:]\n        test_sample = features.reshape(1, -1)\n        \n    # predict opponents move and choose an action\n    next_opp_action_pred = predict_opponent_move(data, test_sample)\n    action = int((next_opp_action_pred + 1) % 3)\n    last_move = {'step': observation.step, 'action': action}\n    return action","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"100\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Example Using The kaggle_environments For Testing Agents<center><h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom kaggle_environments import make, evaluate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"rps\", configuration={\"episodeSteps\": 1000})\n\n# Battle example: invert_my_last_action vs copy_opponent_agent\nenv.run([\"rock.py\", \"hit_the_last_own_action.py\"])\n\nenv.render(mode=\"ipython\", width=800, height=800)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get score for 2 agents in the battle"},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate(\"rps\", [\"rock.py\", \"scissors.py\"], configuration={\"episodeSteps\": 1000})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"101\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Comparison In Battle<center><h2>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_names = [\n    \"hit_the_last_own_action\", \n    \"rock\", \n    \"paper\", \n    \"scissors\", \n    \"copy_opponent\", \n    \"reactionary\", \n    \"counter_reactionary\", \n    \"statistical\", \n    \"nash_equilibrium\",\n    \"markov_agent\", \n    \"memory_patterns\", \n    \"multi_armed_bandit\",\n    \"opponent_transition_matrix\",\n    \"decision_tree_classifier\"\n]\nlist_agents = [agent_name + \".py\" for agent_name in list_names]\n\nscores = np.zeros((len(list_names), len(list_names)), dtype=int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Simulation of battles. It can take some time...\")\n\nfor ind_agent_1 in range(len(list_names)):\n    for ind_agent_2 in range(ind_agent_1 + 1, len(list_names)):\n        print(f\"LOG: {list_names[ind_agent_1]} vs {list_names[ind_agent_2]}\", end=\"\\r\")\n        \n        current_score = evaluate(\n            \"rps\", \n            [list_agents[ind_agent_1], list_agents[ind_agent_2]], \n            configuration={\"episodeSteps\": 1000}\n        )\n        \n        scores[ind_agent_1, ind_agent_2] = current_score[0][0]\n        scores[ind_agent_2, ind_agent_1] = current_score[0][1]\n    \n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores = pd.DataFrame(\n    scores, \n    index=list_names, \n    columns=list_names,\n)\n\n\nplt.figure(figsize=(9, 9))\nsns.heatmap(\n    df_scores, annot=True, cbar=False, cmap='coolwarm', linewidths=1, linecolor='black', fmt=\"d\"\n)\nplt.xticks(rotation=90, fontsize=15)\nplt.yticks(fontsize=15);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"102\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Test Your Own Agent<center><h2>\n"},{"metadata":{},"cell_type":"markdown","source":"Set here your own agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile your_agent.py\n\ndef your_agent(observation, configuration):\n    return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = np.zeros((len(list_names), 1), dtype=int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for ind_agent_1 in range(len(list_names)):\n\n    current_score = evaluate(\n        \"rps\", \n        [\"your_agent.py\", list_agents[ind_agent_1]], \n        configuration={\"episodeSteps\": 1000}\n    )\n\n    scores[ind_agent_1, 0] = current_score[0][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores = pd.DataFrame(\n    scores, \n    index=list_names, \n    columns=[\"your_agent\"],\n)\n\n\nplt.figure(figsize=(2, 10))\nsns.heatmap(\n    df_scores, annot=True, cbar=False, \n    cmap=\"coolwarm\", linewidths=1, linecolor=\"black\", \n    fmt=\"d\", vmin=-500, vmax=500,\n)\nplt.xticks(rotation=90, fontsize=15)\nplt.yticks(rotation=360, fontsize=15);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}